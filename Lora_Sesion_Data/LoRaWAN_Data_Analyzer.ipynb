{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRaWAN Session Data Analyzer\n",
    "\n",
    "Dieses Notebook erm√∂glicht die interaktive Analyse von LoRaWAN Session Daten.\n",
    "\n",
    "## Datenstruktur\n",
    "Die CSV-Dateien enthalten folgende Spalten:\n",
    "- `timestamp`: Zeitstempel der Nachricht\n",
    "- `session_id`: Session-ID\n",
    "- `device_eui`: Device Identifier\n",
    "- `fcnt`: Frame Counter\n",
    "- `rssi_dbm`: Signal Strength (dBm)\n",
    "- `snr_db`: Signal-to-Noise Ratio (dB)\n",
    "- `spreading_factor`: LoRaWAN Spreading Factor\n",
    "- `frequency`: √úbertragungsfrequenz\n",
    "- `device_lat/lon`: GPS-Koordinaten des Devices\n",
    "- `decoded_payload`: Sensor-Daten (JSON)\n",
    "- und weitere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n"
     ]
    }
   ],
   "source": [
    "# Importiere ben√∂tigte Bibliotheken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "# Konfiguration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hilfsfunktionen definiert!\n"
     ]
    }
   ],
   "source": [
    "# Funktionen f√ºr Datenanalyse\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    \"\"\"L√§dt und verarbeitet die CSV-Datei\"\"\"\n",
    "    df = pd.read_csv(file_path)  # CSV mit Komma-Separator\n",
    "    \n",
    "    # Datentyp-Konvertierungen\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['rssi_dbm'] = pd.to_numeric(df['rssi_dbm'], errors='coerce')\n",
    "    df['snr_db'] = pd.to_numeric(df['snr_db'], errors='coerce')\n",
    "    df['frequency'] = pd.to_numeric(df['frequency'], errors='coerce')\n",
    "    df['device_lat'] = pd.to_numeric(df['device_lat'], errors='coerce')\n",
    "    df['device_lon'] = pd.to_numeric(df['device_lon'], errors='coerce')\n",
    "    df['fcnt'] = pd.to_numeric(df['fcnt'], errors='coerce')\n",
    "    \n",
    "    # Parse JSON payload wenn vorhanden\n",
    "    if 'decoded_payload' in df.columns:\n",
    "        df['sensor_data'] = df['decoded_payload'].apply(parse_sensor_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_sensor_data(payload_str):\n",
    "    \"\"\"Parst die JSON-Payload und extrahiert Sensordaten\"\"\"\n",
    "    try:\n",
    "        if pd.isna(payload_str) or payload_str == '':\n",
    "            return None\n",
    "        \n",
    "        # Clean the JSON string\n",
    "        clean_payload = payload_str.replace('\"\"', '\"')\n",
    "        data = json.loads(clean_payload)\n",
    "        \n",
    "        result = {}\n",
    "        if 'd' in data and isinstance(data['d'], list) and len(data['d']) >= 2:\n",
    "            result['temperature'] = data['d'][0]\n",
    "            result['humidity'] = data['d'][1]\n",
    "        \n",
    "        if 'sf' in data:\n",
    "            result['sf_from_payload'] = data['sf']\n",
    "        \n",
    "        if 't' in data:\n",
    "            result['time_counter'] = data['t']\n",
    "        \n",
    "        return result\n",
    "    except (json.JSONDecodeError, KeyError, TypeError):\n",
    "        return None\n",
    "\n",
    "def get_available_files():\n",
    "    \"\"\"Zeigt verf√ºgbare CSV-Dateien an\"\"\"\n",
    "    csv_files = glob(\"*.csv\")\n",
    "    csv_files.sort()\n",
    "    return csv_files\n",
    "\n",
    "def display_file_info(files):\n",
    "    \"\"\"Zeigt Informationen √ºber verf√ºgbare Dateien\"\"\"\n",
    "    print(\"Verf√ºgbare LoRaWAN Session Dateien:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, file in enumerate(files):\n",
    "        file_size = os.path.getsize(file) / 1024  # KB\n",
    "        mod_time = datetime.fromtimestamp(os.path.getmtime(file))\n",
    "        print(f\"{i+1:2d}. {file}\")\n",
    "        print(f\"     Gr√∂√üe: {file_size:.1f} KB, Erstellt: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print()\n",
    "\n",
    "print(\"Hilfsfunktionen definiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verf√ºgbare LoRaWAN Session Dateien:\n",
      "==================================================\n",
      " 1. lorawan_data.csv\n",
      "     Gr√∂√üe: 14.2 KB, Erstellt: 2025-07-07 23:18:09\n",
      "\n",
      " 2. lorawan_session_20250707_204252_526570b6.csv\n",
      "     Gr√∂√üe: 0.6 KB, Erstellt: 2025-07-07 23:18:09\n",
      "\n",
      " 3. lorawan_session_20250707_204441_af09a252 copy.csv\n",
      "     Gr√∂√üe: 0.3 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      " 4. lorawan_session_20250707_210235_7f9c8028 copy.csv\n",
      "     Gr√∂√üe: 0.9 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      " 5. lorawan_session_20250707_210306_bcd879bb copy.csv\n",
      "     Gr√∂√üe: 0.6 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      " 6. lorawan_session_20250707_211812_9c7e7812 copy.csv\n",
      "     Gr√∂√üe: 0.6 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      " 7. lorawan_session_20250707_211833_154b5404 copy.csv\n",
      "     Gr√∂√üe: 0.3 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      " 8. lorawan_session_20250707_211906_f1583e36 copy.csv\n",
      "     Gr√∂√üe: 0.0 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      " 9. lorawan_session_20250707_212614_8d65ca84 copy.csv\n",
      "     Gr√∂√üe: 9.4 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      "10. lorawan_session_20250707_213340_9deb856e copy.csv\n",
      "     Gr√∂√üe: 12.6 KB, Erstellt: 2025-07-08 12:29:59\n",
      "\n",
      "11. lorawan_session_20250707_221922_6119ec60.csv\n",
      "     Gr√∂√üe: 0.3 KB, Erstellt: 2025-07-10 16:33:30\n",
      "\n",
      "12. lorawan_session_20250707_221949_ca42b3c2.csv\n",
      "     Gr√∂√üe: 202.2 KB, Erstellt: 2025-07-10 16:33:30\n",
      "\n",
      "13. lorawan_session_20250707_235158_a9a2044d.csv\n",
      "     Gr√∂√üe: 13.3 KB, Erstellt: 2025-07-10 16:33:30\n",
      "\n",
      "14. lorawan_session_20250715_134658_7ebbbea9.csv\n",
      "     Gr√∂√üe: 8.4 KB, Erstellt: 2025-07-15 16:40:28\n",
      "\n",
      "15. lorawan_session_20250715_141939_a7304177.csv\n",
      "     Gr√∂√üe: 2.3 KB, Erstellt: 2025-07-15 16:40:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeige verf√ºgbare Dateien\n",
    "available_files = get_available_files()\n",
    "display_file_info(available_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgew√§hlte Datei: lorawan_session_20250707_211906_f1583e36 copy.csv\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAusgew√§hlte Datei: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Lade die Daten\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     df = \u001b[43mload_and_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDaten erfolgreich geladen: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Datens√§tze\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mload_and_process_data\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_process_data\u001b[39m(file_path):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"L√§dt und verarbeitet die CSV-Datei\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# CSV mit Komma-Separator\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Datentyp-Konvertierungen\u001b[39;00m\n\u001b[32m      8\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# DATEIAUSWAHL - √Ñndern Sie hier die Zahl entsprechend der gew√ºnschten Datei\n",
    "selected_file_index = 8  # √Ñndern Sie diese Zahl (1-8) um eine andere Datei zu w√§hlen\n",
    "\n",
    "if 1 <= selected_file_index <= len(available_files):\n",
    "    selected_file = available_files[selected_file_index - 1]\n",
    "    print(f\"Ausgew√§hlte Datei: {selected_file}\")\n",
    "    \n",
    "    # Lade die Daten\n",
    "    df = load_and_process_data(selected_file)\n",
    "    print(f\"\\nDaten erfolgreich geladen: {len(df)} Datens√§tze\")\n",
    "else:\n",
    "    print(f\"Ung√ºltige Auswahl! Bitte w√§hlen Sie eine Zahl zwischen 1 und {len(available_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grundlegende Daten√ºbersicht\n",
    "print(\"=\" * 60)\n",
    "print(\"DATEN√úBERSICHT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Dateiname: {selected_file}\")\n",
    "print(f\"Anzahl Datens√§tze: {len(df)}\")\n",
    "print(f\"Zeitraum: {df['timestamp'].min()} bis {df['timestamp'].max()}\")\n",
    "print(f\"Dauer: {df['timestamp'].max() - df['timestamp'].min()}\")\n",
    "print(f\"Unique Devices: {df['device_eui'].nunique()}\")\n",
    "print(f\"Unique Sessions: {df['session_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nSpalten im Datensatz:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erste Zeilen der Daten anzeigen\n",
    "print(\"ERSTE 5 DATENS√ÑTZE:\")\n",
    "print(\"=\" * 60)\n",
    "display_columns = ['timestamp', 'fcnt', 'rssi_dbm', 'snr_db', 'spreading_factor', 'frequency', 'device_lat', 'device_lon']\n",
    "df[display_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensordaten extrahieren und anzeigen\n",
    "sensor_data_expanded = pd.json_normalize(df['sensor_data'].dropna())\n",
    "\n",
    "if not sensor_data_expanded.empty:\n",
    "    print(\"SENSORDATEN (aus JSON Payload):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Anzahl Datens√§tze mit Sensordaten: {len(sensor_data_expanded)}\")\n",
    "    print(\"\\nSensordaten Statistiken:\")\n",
    "    print(sensor_data_expanded.describe())\n",
    "    \n",
    "    # F√ºge Sensordaten zum Haupt-DataFrame hinzu\n",
    "    df_sensor = df.dropna(subset=['sensor_data']).copy()\n",
    "    df_sensor = df_sensor.reset_index(drop=True)\n",
    "    sensor_data_expanded = sensor_data_expanded.reset_index(drop=True)\n",
    "    \n",
    "    for col in sensor_data_expanded.columns:\n",
    "        df_sensor[col] = sensor_data_expanded[col]\n",
    "    \n",
    "    print(\"\\nSensordaten wurden zum DataFrame hinzugef√ºgt!\")\n",
    "else:\n",
    "    print(\"Keine Sensordaten gefunden.\")\n",
    "    df_sensor = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistische Analyse der √úbertragungsqualit√§t\n",
    "print(\"√úBERTRAGUNGSQUALIT√ÑT ANALYSE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# RSSI Statistiken\n",
    "rssi_stats = df['rssi_dbm'].describe()\n",
    "print(\"RSSI (Received Signal Strength Indicator) Statistiken:\")\n",
    "print(rssi_stats)\n",
    "\n",
    "# SNR Statistiken\n",
    "snr_stats = df['snr_db'].describe()\n",
    "print(\"\\nSNR (Signal-to-Noise Ratio) Statistiken:\")\n",
    "print(snr_stats)\n",
    "\n",
    "# Spreading Factor Verteilung\n",
    "print(\"\\nSpreading Factor Verteilung:\")\n",
    "sf_counts = df['spreading_factor'].value_counts().sort_index()\n",
    "print(sf_counts)\n",
    "\n",
    "# Frequency Verteilung\n",
    "print(\"\\nFrequency Verteilung:\")\n",
    "freq_counts = df['frequency'].value_counts().sort_index()\n",
    "print(freq_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierungen erstellen\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle(f'LoRaWAN Session Analysis - {selected_file}', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. RSSI √ºber Zeit\n",
    "axes[0, 0].plot(df['timestamp'], df['rssi_dbm'], 'b-', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 0].set_title('RSSI √ºber Zeit', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Zeit')\n",
    "axes[0, 0].set_ylabel('RSSI (dBm)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. SNR √ºber Zeit\n",
    "axes[0, 1].plot(df['timestamp'], df['snr_db'], 'r-', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 1].set_title('SNR √ºber Zeit', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Zeit')\n",
    "axes[0, 1].set_ylabel('SNR (dB)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Spreading Factor Verteilung\n",
    "sf_counts.plot(kind='bar', ax=axes[1, 0], color='green', alpha=0.7)\n",
    "axes[1, 0].set_title('Spreading Factor Verteilung', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Spreading Factor')\n",
    "axes[1, 0].set_ylabel('Anzahl')\n",
    "axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 4. RSSI vs SNR Scatter\n",
    "scatter = axes[1, 1].scatter(df['rssi_dbm'], df['snr_db'], c=df.index, cmap='viridis', alpha=0.7)\n",
    "axes[1, 1].set_title('RSSI vs SNR Correlation', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('RSSI (dBm)')\n",
    "axes[1, 1].set_ylabel('SNR (dB)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1, 1], label='Zeitsequenz')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensordaten Visualisierung (falls verf√ºgbar)\n",
    "if 'temperature' in df_sensor.columns and 'humidity' in df_sensor.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Sensordaten Analyse', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Temperatur √ºber Zeit\n",
    "    axes[0, 0].plot(df_sensor['timestamp'], df_sensor['temperature'], 'orange', linewidth=2, marker='o', markersize=3)\n",
    "    axes[0, 0].set_title('Temperatur √ºber Zeit', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Zeit')\n",
    "    axes[0, 0].set_ylabel('Temperatur (¬∞C)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Feuchtigkeit √ºber Zeit\n",
    "    axes[0, 1].plot(df_sensor['timestamp'], df_sensor['humidity'], 'cyan', linewidth=2, marker='o', markersize=3)\n",
    "    axes[0, 1].set_title('Feuchtigkeit √ºber Zeit', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Zeit')\n",
    "    axes[0, 1].set_ylabel('Feuchtigkeit (%)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Temperatur Histogramm\n",
    "    axes[1, 0].hist(df_sensor['temperature'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 0].set_title('Temperatur Verteilung', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Temperatur (¬∞C)')\n",
    "    axes[1, 0].set_ylabel('H√§ufigkeit')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feuchtigkeit Histogramm\n",
    "    axes[1, 1].hist(df_sensor['humidity'], bins=20, alpha=0.7, color='cyan', edgecolor='black')\n",
    "    axes[1, 1].set_title('Feuchtigkeit Verteilung', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Feuchtigkeit (%)')\n",
    "    axes[1, 1].set_ylabel('H√§ufigkeit')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sensordaten Statistiken\n",
    "    print(\"\\nSENSORDATEN STATISTIKEN:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Temperatur: {df_sensor['temperature'].mean():.2f}¬∞C ¬± {df_sensor['temperature'].std():.2f}¬∞C\")\n",
    "    print(f\"Bereich: {df_sensor['temperature'].min():.2f}¬∞C - {df_sensor['temperature'].max():.2f}¬∞C\")\n",
    "    print(f\"\\nFeuchtigkeit: {df_sensor['humidity'].mean():.2f}% ¬± {df_sensor['humidity'].std():.2f}%\")\n",
    "    print(f\"Bereich: {df_sensor['humidity'].min():.2f}% - {df_sensor['humidity'].max():.2f}%\")\n",
    "else:\n",
    "    print(\"Keine Sensordaten (Temperatur/Feuchtigkeit) verf√ºgbar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erweiterte Analyse: Korrelationen\n",
    "print(\"KORRELATIONSANALYSE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Numerische Spalten f√ºr Korrelation\n",
    "numeric_cols = ['rssi_dbm', 'snr_db', 'fcnt', 'frequency']\n",
    "if 'temperature' in df_sensor.columns:\n",
    "    numeric_cols.extend(['temperature', 'humidity'])\n",
    "\n",
    "# Korrelationsmatrix\n",
    "correlation_data = df_sensor[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_data, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'label': 'Korrelation'})\n",
    "plt.title('Korrelationsmatrix der Messwerte', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSt√§rkste Korrelationen:\")\n",
    "# Finde die st√§rksten Korrelationen (au√üer Diagonal)\n",
    "corr_pairs = []\n",
    "for i in range(len(correlation_data.columns)):\n",
    "    for j in range(i+1, len(correlation_data.columns)):\n",
    "        corr_value = correlation_data.iloc[i, j]\n",
    "        if not np.isnan(corr_value):\n",
    "            corr_pairs.append((correlation_data.columns[i], correlation_data.columns[j], corr_value))\n",
    "\n",
    "# Sortiere nach absoluter Korrelation\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "for i, (var1, var2, corr) in enumerate(corr_pairs[:5]):\n",
    "    print(f\"{i+1}. {var1} ‚Üî {var2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netzwerkqualit√§ts-Dashboard\n",
    "def analyze_network_quality(df):\n",
    "    \"\"\"Analysiert die Netzwerkqualit√§t basierend auf RSSI und SNR\"\"\"\n",
    "    \n",
    "    # Qualit√§tskategorien definieren\n",
    "    def rssi_category(rssi):\n",
    "        if rssi >= -70:\n",
    "            return 'Excellent'\n",
    "        elif rssi >= -80:\n",
    "            return 'Good'\n",
    "        elif rssi >= -90:\n",
    "            return 'Fair'\n",
    "        else:\n",
    "            return 'Poor'\n",
    "    \n",
    "    def snr_category(snr):\n",
    "        if snr >= 10:\n",
    "            return 'Excellent'\n",
    "        elif snr >= 5:\n",
    "            return 'Good'\n",
    "        elif snr >= 0:\n",
    "            return 'Fair'\n",
    "        else:\n",
    "            return 'Poor'\n",
    "    \n",
    "    df['rssi_quality'] = df['rssi_dbm'].apply(rssi_category)\n",
    "    df['snr_quality'] = df['snr_db'].apply(snr_category)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analysiere Netzwerkqualit√§t\n",
    "df_quality = analyze_network_quality(df.copy())\n",
    "\n",
    "# Visualisiere Qualit√§tsverteilung\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Netzwerkqualit√§ts-Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# RSSI Qualit√§tsverteilung\n",
    "rssi_quality_counts = df_quality['rssi_quality'].value_counts()\n",
    "colors = ['green', 'orange', 'red', 'darkred']\n",
    "quality_order = ['Excellent', 'Good', 'Fair', 'Poor']\n",
    "rssi_quality_counts = rssi_quality_counts.reindex(quality_order, fill_value=0)\n",
    "\n",
    "axes[0].pie(rssi_quality_counts.values, labels=rssi_quality_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors[:len(rssi_quality_counts)], startangle=90)\n",
    "axes[0].set_title('RSSI Qualit√§tsverteilung', fontweight='bold')\n",
    "\n",
    "# SNR Qualit√§tsverteilung\n",
    "snr_quality_counts = df_quality['snr_quality'].value_counts()\n",
    "snr_quality_counts = snr_quality_counts.reindex(quality_order, fill_value=0)\n",
    "\n",
    "axes[1].pie(snr_quality_counts.values, labels=snr_quality_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors[:len(snr_quality_counts)], startangle=90)\n",
    "axes[1].set_title('SNR Qualit√§tsverteilung', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Qualit√§tsstatistiken ausgeben\n",
    "print(\"\\nNETZWERKQUALIT√ÑT ZUSAMMENFASSUNG\")\n",
    "print(\"=\" * 60)\n",
    "print(\"RSSI Qualit√§tsverteilung:\")\n",
    "for quality, count in rssi_quality_counts.items():\n",
    "    percentage = (count / len(df_quality)) * 100\n",
    "    print(f\"  {quality}: {count} Nachrichten ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nSNR Qualit√§tsverteilung:\")\n",
    "for quality, count in snr_quality_counts.items():\n",
    "    percentage = (count / len(df_quality)) * 100\n",
    "    print(f\"  {quality}: {count} Nachrichten ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenfassung und Export-Optionen\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSEZUSAMMENFASSUNG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'Dateiname': selected_file,\n",
    "    'Analysezeitpunkt': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'Gesamtnachrichten': len(df),\n",
    "    'Zeitraum': f\"{df['timestamp'].min()} bis {df['timestamp'].max()}\",\n",
    "    'Durchschnittliche_RSSI': f\"{df['rssi_dbm'].mean():.2f} dBm\",\n",
    "    'Durchschnittliche_SNR': f\"{df['snr_db'].mean():.2f} dB\",\n",
    "    'H√§ufigster_SF': df['spreading_factor'].mode().iloc[0],\n",
    "    'Unique_Devices': df['device_eui'].nunique(),\n",
    "    'Qualit√§t_Excellent_RSSI': f\"{(rssi_quality_counts.get('Excellent', 0) / len(df_quality) * 100):.1f}%\",\n",
    "    'Qualit√§t_Excellent_SNR': f\"{(snr_quality_counts.get('Excellent', 0) / len(df_quality) * 100):.1f}%\"\n",
    "}\n",
    "\n",
    "if 'temperature' in df_sensor.columns:\n",
    "    summary.update({\n",
    "        'Durchschnittliche_Temperatur': f\"{df_sensor['temperature'].mean():.2f}¬∞C\",\n",
    "        'Durchschnittliche_Feuchtigkeit': f\"{df_sensor['humidity'].mean():.2f}%\"\n",
    "    })\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key.replace('_', ' ')}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPORT-OPTIONEN\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Sie k√∂nnen die Analyseergebnisse exportieren:\")\n",
    "print(\"1. DataFrame als CSV: df.to_csv('analysis_results.csv')\")\n",
    "print(\"2. Zusammenfassung als JSON: import json; json.dump(summary, open('summary.json', 'w'))\")\n",
    "print(\"3. Grafiken als PNG: plt.savefig('analysis_plots.png', dpi=300, bbox_inches='tight')\")\n",
    "\n",
    "print(\"\\nAnalyse abgeschlossen! üéâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionale Detailanalyse - Zeitreihenanalyse\n",
    "print(\"ZEITREIHENANALYSE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Zeitintervalle zwischen Nachrichten\n",
    "df_sorted = df.sort_values('timestamp')\n",
    "time_diffs = df_sorted['timestamp'].diff().dt.total_seconds()\n",
    "time_diffs = time_diffs.dropna()\n",
    "\n",
    "print(f\"Durchschnittliches Intervall zwischen Nachrichten: {time_diffs.mean():.2f} Sekunden\")\n",
    "print(f\"Minimum Intervall: {time_diffs.min():.2f} Sekunden\")\n",
    "print(f\"Maximum Intervall: {time_diffs.max():.2f} Sekunden\")\n",
    "\n",
    "# Visualisierung der Zeitintervalle\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(time_diffs, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Verteilung der Zeitintervalle zwischen Nachrichten', fontweight='bold')\n",
    "plt.xlabel('Zeitintervall (Sekunden)')\n",
    "plt.ylabel('H√§ufigkeit')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(time_diffs)), time_diffs, 'b-', alpha=0.7)\n",
    "plt.title('Zeitintervalle √ºber die Session', fontweight='bold')\n",
    "plt.xlabel('Nachrichtennummer')\n",
    "plt.ylabel('Zeitintervall (Sekunden)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
